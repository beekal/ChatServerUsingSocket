{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFX : Production Scale ML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNVdQBdA4nOxmA5rxmRpttZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beekal/ChatServerUsingSocket/blob/master/TFX_Production_Scale_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfiXx2p98rNY",
        "colab_type": "text"
      },
      "source": [
        "## TFX : Why ?\n",
        "  - Provides end to end research to prod level ML solution , including Model versioning.\n",
        "  - Bundles all the task into a package ranging from\n",
        "    - Reading a data\n",
        "    - Preprocessing\n",
        "    - Model building / validation\n",
        "    - Model Deployment\n",
        "    - Model Versioning with rollback feature\n",
        "\n",
        "If we do not use the TFX and a separate model independent preprocessing steps, then we really would not be able to rollback, in case preprocessing step changes with the model version updates.\n",
        "\n",
        "### TFX Components :\n",
        "A TFX consists of following components which we will discuss here\n",
        "  - ExampleGen : Read data into TFX pipeline\n",
        "  - StatisticsGen : Calculate exploratory Statistics about the data\n",
        "  - SchemaGen : Create a data schema based on the Statistics\n",
        "  - ExampleValidator: Analyse the data for abnormalities / inconsistencies\n",
        "  - Transform: Perform necessary transformation in the data\n",
        "  - Trainer : Trains the model\n",
        "  - Evaluator : Evaluate the model performance to determine its readiness for deployment/ discard\n",
        "  - Pusher : Deploys model to the production\n",
        "  ![alt text](https://www.tensorflow.org/tfx/guide/images/diag_all.png)\n",
        "\n",
        "\n",
        "## Dependency Requirements :\n",
        "  - Apache Beam :Develop in single node, run in multi-node \n",
        "  We want our ML to run parallely for  greater speed while also being scalable. E.g we would likely develop the Ml model  on a single computer, however when we want it on a prod, we would like to run it on a multi-node cluster environment to serve a lot of parallel requests  with low latency.  Apache  Beam provides this abstraction i.e whatever we research / develop in a single nodeis easily scalable  to multi-node cluster, without any extra work/effort or code modification.\n",
        "  - Apache Airflow / Kubeflow : Deploy, Scale and manage containerised ML application automatically. Some example Cases Airflow/ Kubeflow handles\n",
        "    - Install required libraries in 100 node clusters\n",
        "    - then Deploy ML model to all of them\n",
        "    - Receiving tremendous volume of request for ML model, scale them up..\n",
        "    - Terrible disaster 50 nodes have gone down, we need to bring another 50 uoo to compensate.\n",
        "    - Efficiently utilise CPU/ GPU and minismise cost\n",
        "\n",
        "REF : https://docs.agilestacks.com/article/gkyq26pzmr-creating-an-ml-pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0ao80eZ3sKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}